{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMQZhmY4CIzj",
        "outputId": "8083b063-038d-4631-e19a-f2f137042a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n",
            "Epoch 1/10\n",
            "250/250 [==============================] - 16s 47ms/step - loss: 0.4112 - accuracy: 0.7843 - val_loss: 0.1771 - val_accuracy: 0.9215\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.0795 - accuracy: 0.9650 - val_loss: 0.0577 - val_accuracy: 0.9710\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 11s 44ms/step - loss: 0.0464 - accuracy: 0.9720 - val_loss: 0.0451 - val_accuracy: 0.9705\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.0404 - accuracy: 0.9750 - val_loss: 0.0441 - val_accuracy: 0.9705\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 11s 44ms/step - loss: 0.0375 - accuracy: 0.9724 - val_loss: 0.0430 - val_accuracy: 0.9675\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 9s 34ms/step - loss: 0.0526 - accuracy: 0.9685 - val_loss: 0.0444 - val_accuracy: 0.9715\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 10s 41ms/step - loss: 0.0386 - accuracy: 0.9744 - val_loss: 0.0416 - val_accuracy: 0.9680\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 10s 41ms/step - loss: 0.0361 - accuracy: 0.9756 - val_loss: 0.0420 - val_accuracy: 0.9705\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 9s 38ms/step - loss: 0.0360 - accuracy: 0.9739 - val_loss: 0.0420 - val_accuracy: 0.9680\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 11s 45ms/step - loss: 0.0358 - accuracy: 0.9747 - val_loss: 0.0413 - val_accuracy: 0.9675\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Phrase: This product is amazing!\n",
            "Predicted Sentiment: positive\n",
            "\n",
            "Phrase: I regret buying this.\n",
            "Predicted Sentiment: negative\n",
            "\n",
            "Phrase: It is just average.\n",
            "Predicted Sentiment: negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "\n",
        "# Read sentiment data from CSV file\n",
        "data = pd.read_csv('/content/drive/My Drive/sentiment_data.csv')\n",
        "\n",
        "# Separate phrases and sentiments from the data\n",
        "phrases = data['Phrase'].values\n",
        "sentiments = data['Sentiment'].values\n",
        "\n",
        "# Tokenize the phrases\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(phrases)\n",
        "sequences = tokenizer.texts_to_sequences(phrases)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Convert sentiments to categorical labels\n",
        "sentiment_labels = np.unique(sentiments)\n",
        "encoded_sentiments = np.array([np.where(sentiment_labels == sentiment)[0][0] for sentiment in sentiments])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(padded_sequences) * split_ratio)\n",
        "\n",
        "x_train = padded_sequences[:split_index]\n",
        "y_train = encoded_sentiments[:split_index]\n",
        "\n",
        "x_test = padded_sequences[split_index:]\n",
        "y_test = encoded_sentiments[split_index:]\n",
        "\n",
        "# Define the RNN model\n",
        "embedding_dim = 100\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(len(sentiment_labels), activation='softmax'))\n",
        "\n",
        "# Compile and train the model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\n",
        "\n",
        "# Make predictions on new data\n",
        "new_phrases = ['This product is amazing!', 'I regret buying this.', 'It is just average.']\n",
        "new_sequences = tokenizer.texts_to_sequences(new_phrases)\n",
        "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_sequence_length)\n",
        "predictions = model.predict(new_padded_sequences)\n",
        "\n",
        "# Convert predictions to sentiment labels\n",
        "predicted_sentiments = [sentiment_labels[np.argmax(prediction)] for prediction in predictions]\n",
        "\n",
        "# Print the predicted sentiments\n",
        "for phrase, sentiment in zip(new_phrases, predicted_sentiments):\n",
        "    print(f\"Phrase: {phrase}\")\n",
        "    print(f\"Predicted Sentiment: {sentiment}\")\n",
        "    print()\n"
      ]
    }
  ]
}